{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Disclaimer :\nthis notebook is not published with good performance, the notebook aims to  propose an approach based on Spark to introduce people to Distributed Computing and to encourage this type of implementation, that is representative of the industry reality.\n\nIn this notebook, we'll try to introduce you  pyspark which is an interface for Apache Spark ( dedicated for distributed computation ) in Python.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport random\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-07T06:51:44.168743Z","iopub.execute_input":"2022-08-07T06:51:44.169661Z","iopub.status.idle":"2022-08-07T06:51:44.200677Z","shell.execute_reply.started":"2022-08-07T06:51:44.169551Z","shell.execute_reply":"2022-08-07T06:51:44.199650Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"\n\nThe power of Pyspark is its simplicity, because we don't need to manipulate / use Resilient Distributed Dataset ( RDD ) which are the basic unit of Spark that are distributed on the cluster, pyspark handle all of this. However, if you want to program at this level you can easily do it.\n\n## Resilient Distributed Dataset\n-  An RDD (Resilient Distributed Dataset) is the basic abstraction of Spark representing an unchanging set of elements partitioned across cluster nodes, allowing parallel computation.\n- It has 3 advantages:\n    - Performance.\n    - Consistency.\n    - Fault Tolerance\n\nPyspark supports most of Spark features such as Dataframe / SparkSQL / MLlib. We will not introduce Spark Streaming in this tutorial.\n\n## SparkSQL \n- Simply said, sparkSQL allows  to query structured data inside Spark programs. It acts as a distributed SQL query engine for fast data retrieving and processing. We can  also use SQL's like code creating a temporary view as we will see at the end of the tutorial\n- It provides a programming abstraction called DataFrame:\n    -  Just as Python Dataframe, pyspark allows us to use this structure which is distributable across multiple machine of the cluster\n\n## MLlib\n- MLlib is a scalable machine learning library that provides a uniform set of high-level APIs that help users create and tune practical machine learning pipelines. For this competition, we'll use trees algorithms.\n\n","metadata":{}},{"cell_type":"code","source":"# Install pyspark\n!pip3 install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:51:44.202289Z","iopub.execute_input":"2022-08-07T06:51:44.202816Z","iopub.status.idle":"2022-08-07T06:52:35.784709Z","shell.execute_reply.started":"2022-08-07T06:51:44.202782Z","shell.execute_reply":"2022-08-07T06:52:35.782927Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=3b02e5cc91158c80712e3a9cc180d6bc3f14933f1fca1da615042e9067b65536\n  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pyspark\nimport pyspark.sql.functions as F\nimport pyspark.sql.types\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:52:35.787720Z","iopub.execute_input":"2022-08-07T06:52:35.788232Z","iopub.status.idle":"2022-08-07T06:52:35.870510Z","shell.execute_reply.started":"2022-08-07T06:52:35.788163Z","shell.execute_reply":"2022-08-07T06:52:35.869288Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"SparkSession is the entry point to Spark/Pyspark to work with RDD/Dataframe. We can also configure properties like :\n\n - `spark.driver.memory` : Amount of memory to use for the driver process where the Session is initialized. I initialize it by default at 16gb         \n - `spark.sql.shuffle.partitions` :  number of partitions to use when shuffling data for joins or aggregations. In this example we partitioned the data into 150 partitions.","metadata":{}},{"cell_type":"code","source":"spark = (SparkSession.builder.master(\"local[*]\")\n                    .config(\"spark.driver.memory\",\"16g\")\n                    .config(\"spark.sql.shuffle.partitions\",20)\n                    .appName('PysparkIsAllYouNeed')\n                    .getOrCreate())\n","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:52:35.872126Z","iopub.execute_input":"2022-08-07T06:52:35.872842Z","iopub.status.idle":"2022-08-07T06:52:41.544269Z","shell.execute_reply.started":"2022-08-07T06:52:35.872805Z","shell.execute_reply":"2022-08-07T06:52:41.543165Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"22/08/07 06:52:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"def sample_df(n_cols,fraction_rows,path):\n    print(\"using PySpark ...\")\n    parDF1=spark.read.parquet(path)\n    cols_list = [\"customer_ID\"] + random.sample(parDF1.columns[1:-1],n_cols) + ['target']\n    \n    parDF1 = parDF1.select(*cols_list).sample(fraction=fraction_rows)\n    \n    \n    return parDF1","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:52:41.550329Z","iopub.execute_input":"2022-08-07T06:52:41.552541Z","iopub.status.idle":"2022-08-07T06:52:41.560626Z","shell.execute_reply.started":"2022-08-07T06:52:41.552487Z","shell.execute_reply":"2022-08-07T06:52:41.559500Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"I observe that using all the columns make the kernel crash, so I decide to randomly select a subset of columns while looking for future solution.","metadata":{}},{"cell_type":"code","source":"train_sample = sample_df(n_cols=40,fraction_rows=0.2,path=\"../input/amex-parquet/train_data.parquet\")\ntrain_sample.count(),len(train_sample.columns)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:52:41.563429Z","iopub.execute_input":"2022-08-07T06:52:41.563758Z","iopub.status.idle":"2022-08-07T06:52:50.277964Z","shell.execute_reply.started":"2022-08-07T06:52:41.563729Z","shell.execute_reply":"2022-08-07T06:52:50.276697Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"using PySpark ...\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(1105416, 42)"},"metadata":{}}]},{"cell_type":"code","source":"cols_used = train_sample.columns","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:52:50.280297Z","iopub.execute_input":"2022-08-07T06:52:50.283528Z","iopub.status.idle":"2022-08-07T06:52:50.291877Z","shell.execute_reply.started":"2022-08-07T06:52:50.283477Z","shell.execute_reply":"2022-08-07T06:52:50.290512Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom sklearn.metrics import confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:52:50.296057Z","iopub.execute_input":"2022-08-07T06:52:50.296551Z","iopub.status.idle":"2022-08-07T06:52:51.002638Z","shell.execute_reply.started":"2022-08-07T06:52:50.296505Z","shell.execute_reply":"2022-08-07T06:52:51.001627Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"For the sake of the tutorial, I decide to apply a very simple preprocessing and aggregation. You can easily perform better with further preprocessing","metadata":{}},{"cell_type":"markdown","source":"### Aggregation function","metadata":{}},{"cell_type":"code","source":"def agg_data(train_sample,option=\"train\"):\n    # Select only numerical columns for aggregation\n    if option ==\"train\":\n        numerical_list = [item[0] for item in train_sample.dtypes[1:-1] if not  item[1].startswith('string') ]\n\n\n    else:\n        numerical_list = [item[0] for item in train_sample.dtypes[1:] if not  item[1].startswith('string') ]\n    print(numerical_list)\n    sum_agg = {x: \"sum\" for x in numerical_list   if x is not train_sample.columns[0] }\n    min_agg = {x: \"min\" for x in numerical_list if x is not train_sample.columns[0] }\n    std_agg = {x: \"stddev_samp\" for x in numerical_list if x is not train_sample.columns[0] }\n    #variance_agg = {x: \"variance\" for x in numerical_list if x is not train_sample.columns[0] }\n    avg_agg = {x: \"mean\" for x in numerical_list if x is not train_sample.columns[0] }\n    if option ==\"train\":\n        max_agg = {x: \"max\" for x in numerical_list + ['target'] if x is not train_sample.columns[0] }\n\n    else:\n        max_agg = {x: \"max\" for x in numerical_list if x is not train_sample.columns[0] }\n\n    sum_df = train_sample.groupBy(\"customer_ID\").agg(sum_agg).withColumnRenamed(\"customer_ID\",\"customer_ID_1\")\n    min_df = train_sample.groupBy(\"customer_ID\").agg(min_agg).withColumnRenamed(\"customer_ID\",\"customer_ID_2\")\n    std_df = train_sample.groupBy(\"customer_ID\").agg(std_agg).withColumnRenamed(\"customer_ID\",\"customer_ID_3\")\n    #variance_df = train_sample.groupBy(\"customer_ID\").agg(variance_agg).withColumnRenamed(\"customer_ID\",\"customer_ID_4\")\n    avg_df = train_sample.groupBy(\"customer_ID\").agg(avg_agg).withColumnRenamed(\"customer_ID\",\"customer_ID_5\")\n    max_df = train_sample.groupBy(\"customer_ID\").agg(max_agg).withColumnRenamed(\"customer_ID\",\"customer_ID_6\")\n\n    join1 = sum_df.join(min_df, sum_df.customer_ID_1 == min_df.customer_ID_2, 'inner').select(\"*\")\n    join2 = join1.join(std_df, join1.customer_ID_1 == std_df.customer_ID_3, 'inner').select(\"*\")\n    #join3 = join2.join(variance_df, join2.customer_ID_1 == variance_df.customer_ID_4, 'inner').select(\"*\")\n\n    join4 = join2.join(avg_df, join2.customer_ID_1 == avg_df.customer_ID_5, 'inner').select(\"*\")\n\n    join_final = join4.join(max_df, join4.customer_ID_1 == max_df.customer_ID_6, 'inner').select(\"*\")\n\n    join_final = join_final.drop(*[\"customer_ID_{}\".format(x) for x in range(2,7)]).withColumnRenamed(\"customer_ID_1\",\"customer_ID\")\n\n    return join_final","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:52:51.004339Z","iopub.execute_input":"2022-08-07T06:52:51.005079Z","iopub.status.idle":"2022-08-07T06:52:51.021749Z","shell.execute_reply.started":"2022-08-07T06:52:51.005030Z","shell.execute_reply":"2022-08-07T06:52:51.020605Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing ","metadata":{}},{"cell_type":"markdown","source":"It is important to note that MLlib models inputs should be in this form ( X,Y ) where X represent the vectorized inputs. For this, we simply need to use `VectorAssembler`.","metadata":{}},{"cell_type":"code","source":"def preprocess_data(df,option=\"train\"):\n\n    if option ==\"train\":\n        agg_df = agg_data(df,option=\"train\")\n        print(\"training ...\")\n        agg_df = agg_df.withColumnRenamed(\"max(target)\", \"target\")\n        input_cols = agg_df.select(\"*\").drop(\"customer_ID\",\"target\").columns\n        va = VectorAssembler(inputCols = input_cols, outputCol='features',handleInvalid = \"keep\")\n        va_df = va.transform(agg_df)\n        va_df = va_df.select(['features', 'target'])  \n    else:\n        print(\"testing ...\")\n        agg_df = agg_data(df,option=\"test\")\n        input_cols = agg_df.select(\"*\").drop(\"customer_ID\").columns\n        va = VectorAssembler(inputCols = input_cols, outputCol='features',handleInvalid = \"keep\")\n        va_df = va.transform(agg_df)\n        va_df = va_df.select(['customer_ID','features']) \n    \n          \n    return va_df","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:52:51.023354Z","iopub.execute_input":"2022-08-07T06:52:51.024396Z","iopub.status.idle":"2022-08-07T06:52:51.039384Z","shell.execute_reply.started":"2022-08-07T06:52:51.024323Z","shell.execute_reply":"2022-08-07T06:52:51.038234Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_data_agg_vectorized  = preprocess_data(train_sample,\"train\")\ntrain_data_agg_vectorized.count(),len(train_data_agg_vectorized.columns)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:52:51.040709Z","iopub.execute_input":"2022-08-07T06:52:51.041522Z","iopub.status.idle":"2022-08-07T06:53:02.564029Z","shell.execute_reply.started":"2022-08-07T06:52:51.041485Z","shell.execute_reply":"2022-08-07T06:53:02.562688Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['D_82', 'D_121', 'B_6', 'R_28', 'S_18', 'D_88', 'R_27', 'B_2', 'D_132', 'S_16', 'D_138', 'D_113', 'S_13', 'R_18', 'R_15', 'D_70', 'B_12', 'B_39', 'B_26', 'D_73', 'R_17', 'B_37', 'D_39', 'B_13', 'D_139', 'D_109', 'D_81', 'D_123', 'B_27', 'D_83', 'S_22', 'D_72', 'S_9', 'D_96', 'R_13', 'B_41', 'D_143', 'S_24']\ntraining ...\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(417202, 2)"},"metadata":{}}]},{"cell_type":"code","source":"# Split the data randomly\n(train, valid) = train_data_agg_vectorized.randomSplit([0.7, 0.3])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:53:02.573733Z","iopub.execute_input":"2022-08-07T06:53:02.576920Z","iopub.status.idle":"2022-08-07T06:53:02.661367Z","shell.execute_reply.started":"2022-08-07T06:53:02.576861Z","shell.execute_reply":"2022-08-07T06:53:02.659960Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Model & evaluation","metadata":{}},{"cell_type":"code","source":"## Use Random Forest \n'''\ndtc = RandomForestClassifier(numTrees=20,\n      maxDepth=8,subsamplingRate=0.8,\n      featureSubsetStrategy= \"sqrt\",\n      featuresCol=\"features\", labelCol=\"target\")\n'''\ndtc = DecisionTreeClassifier(maxDepth=10,featuresCol=\"features\", labelCol=\"target\")\nmodel = dtc.fit(train)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:53:02.662684Z","iopub.execute_input":"2022-08-07T06:53:02.663102Z","iopub.status.idle":"2022-08-07T06:56:45.243710Z","shell.execute_reply.started":"2022-08-07T06:53:02.663059Z","shell.execute_reply":"2022-08-07T06:56:45.241896Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"[Stage 14:=====================================================>  (25 + 1) / 26]\r","output_type":"stream"},{"name":"stdout","text":"22/08/07 06:53:47 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","output_type":"stream"},{"name":"stderr","text":"[Stage 19:=====================================================>  (25 + 1) / 26]\r","output_type":"stream"},{"name":"stdout","text":"22/08/07 06:54:16 WARN DAGScheduler: Broadcasting large task binary with size 1033.5 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"22/08/07 06:56:37 WARN DAGScheduler: Broadcasting large task binary with size 1005.1 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"22/08/07 06:56:39 WARN DAGScheduler: Broadcasting large task binary with size 1021.0 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"22/08/07 06:56:40 WARN DAGScheduler: Broadcasting large task binary with size 1049.7 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"22/08/07 06:56:42 WARN DAGScheduler: Broadcasting large task binary with size 1100.3 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"Because of the unbalanced character of the dataset, we use ``areaUnderROC`` using the `BinaryClassificationEvaluator`.\n\nIts also important to note that `.collect()` will gather/collect all the data that are distributed accross the cluster to this kernel. It's usually not recommended to use this method because local RAM can't handle this amount of data generally.","metadata":{}},{"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"target\",metricName=\"areaUnderROC\")\npred = model.transform(valid)\nacc = evaluator.evaluate(pred)\n \nprint(\"Prediction auc: \", acc)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:56:45.246958Z","iopub.execute_input":"2022-08-07T06:56:45.247735Z","iopub.status.idle":"2022-08-07T06:57:20.555407Z","shell.execute_reply.started":"2022-08-07T06:56:45.247688Z","shell.execute_reply":"2022-08-07T06:57:20.553644Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"[Stage 131:====================================================>  (25 + 1) / 26]\r","output_type":"stream"},{"name":"stdout","text":"22/08/07 06:57:06 WARN DAGScheduler: Broadcasting large task binary with size 1042.0 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 137:==============================================>        (17 + 3) / 20]\r","output_type":"stream"},{"name":"stdout","text":"Prediction auc:  0.7613264801472656\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"'''\ny_pred=pred.select(\"prediction\").collect()\ny_orig=pred.select(\"target\").collect()\n\ncm = confusion_matrix(y_orig, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\n'''","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:57:20.557127Z","iopub.execute_input":"2022-08-07T06:57:20.557619Z","iopub.status.idle":"2022-08-07T06:57:20.568701Z","shell.execute_reply.started":"2022-08-07T06:57:20.557579Z","shell.execute_reply":"2022-08-07T06:57:20.566914Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'\\ny_pred=pred.select(\"prediction\").collect()\\ny_orig=pred.select(\"target\").collect()\\n\\ncm = confusion_matrix(y_orig, y_pred)\\nprint(\"Confusion Matrix:\")\\nprint(cm)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# Free memory\ndel  train,valid, evaluator, acc, train_sample\ntry :\n    del train_data_agg_vectorized, cm, y_pred, y_orig, pred,\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:57:20.571353Z","iopub.execute_input":"2022-08-07T06:57:20.572429Z","iopub.status.idle":"2022-08-07T06:57:21.288894Z","shell.execute_reply.started":"2022-08-07T06:57:20.572378Z","shell.execute_reply":"2022-08-07T06:57:21.287402Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"test_path = \"../input/amex-parquet/test_data.parquet\"\ntest_cols = cols_used[:-1] # Use the same columns but without the target\ntest_data =spark.read.parquet(test_path).select(*test_cols)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:57:21.290417Z","iopub.execute_input":"2022-08-07T06:57:21.291610Z","iopub.status.idle":"2022-08-07T06:57:21.514748Z","shell.execute_reply.started":"2022-08-07T06:57:21.291564Z","shell.execute_reply":"2022-08-07T06:57:21.513454Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"test_data_agg_vectorized  = preprocess_data(test_data,\"test\")\n\ntest_data_agg_vectorized.count(),len(test_data_agg_vectorized.columns)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:57:21.516170Z","iopub.execute_input":"2022-08-07T06:57:21.516706Z","iopub.status.idle":"2022-08-07T06:57:53.962243Z","shell.execute_reply.started":"2022-08-07T06:57:21.516659Z","shell.execute_reply":"2022-08-07T06:57:53.960480Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"testing ...\n['D_82', 'D_121', 'B_6', 'R_28', 'S_18', 'D_88', 'R_27', 'B_2', 'D_132', 'S_16', 'D_138', 'D_113', 'S_13', 'R_18', 'R_15', 'D_70', 'B_12', 'B_39', 'B_26', 'D_73', 'R_17', 'B_37', 'D_39', 'B_13', 'D_139', 'D_109', 'D_81', 'D_123', 'B_27', 'D_83', 'S_22', 'D_72', 'S_9', 'D_96', 'R_13', 'B_41', 'D_143', 'S_24']\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(924621, 2)"},"metadata":{}}]},{"cell_type":"code","source":"test_pred = model.transform(test_data_agg_vectorized)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:57:53.964215Z","iopub.execute_input":"2022-08-07T06:57:53.964679Z","iopub.status.idle":"2022-08-07T06:57:54.177627Z","shell.execute_reply.started":"2022-08-07T06:57:53.964631Z","shell.execute_reply":"2022-08-07T06:57:54.176271Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"test_pred.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-07T06:57:54.179116Z","iopub.execute_input":"2022-08-07T06:57:54.180259Z","iopub.status.idle":"2022-08-07T07:00:21.313444Z","shell.execute_reply.started":"2022-08-07T06:57:54.180188Z","shell.execute_reply":"2022-08-07T07:00:21.312004Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"[Stage 172:=====================================================> (51 + 1) / 52]\r","output_type":"stream"},{"name":"stdout","text":"22/08/07 07:00:18 WARN DAGScheduler: Broadcasting large task binary with size 1017.6 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 180:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+--------------------+--------------------+-----------------+--------------------+----------+\n|         customer_ID|            features|    rawPrediction|         probability|prediction|\n+--------------------+--------------------+-----------------+--------------------+----------+\n|0000210045da4f81e...|[0.06097930658143...|   [3002.0,635.0]|[0.82540555402804...|       0.0|\n|00003b41e58ede33b...|[0.05114417144795...|  [3122.0,1937.0]|[0.61711800751136...|       0.0|\n|00004ffe6e01e1b68...|[0.07659108197549...|[111749.0,2306.0]|[0.97978168427513...|       0.0|\n|00031400ff14b7d1f...|[0.06699633877724...|[111749.0,2306.0]|[0.97978168427513...|       0.0|\n|0003e5c8c3d0be014...|[0.07874763966538...|[111749.0,2306.0]|[0.97978168427513...|       0.0|\n|00044da6bd1122c29...|[0.08059975429205...|   [7734.0,794.0]|[0.90689493433395...|       0.0|\n|00048ea2f1b75bb67...|[0.06202506215777...|    [199.0,536.0]|[0.27074829931972...|       1.0|\n|000ada1377b0e5267...|[0.08360742888180...|     [10.0,214.0]|[0.04464285714285...|       1.0|\n|000cbba0ecc12fead...|[0.06072052137460...|  [5908.0,2339.0]|[0.71638171456287...|       0.0|\n|000d4de1ea2fe11f8...|[0.06955071742413...|       [16.0,2.0]|[0.88888888888888...|       0.0|\n|000d801ce14c2c93a...|[0.05806354846572...|     [96.0,273.0]|[0.26016260162601...|       1.0|\n|000deb989bb4d57bc...|[0.05978236795635...|[111749.0,2306.0]|[0.97978168427513...|       0.0|\n|00105f7f87e164776...|[0.06062795926118...|   [695.0,1089.0]|[0.38957399103139...|       1.0|\n|0011ffd8bb3c8fe1f...|[0.05866274540312...|[111749.0,2306.0]|[0.97978168427513...|       0.0|\n|0012a607ee3838de5...|[0.06336843664757...|[111749.0,2306.0]|[0.97978168427513...|       0.0|\n|001527367423b1872...|[0.04365163241163...|   [7734.0,794.0]|[0.90689493433395...|       0.0|\n|001689f6cb73ee973...|[0.07901332085020...|    [199.0,536.0]|[0.27074829931972...|       1.0|\n|0018d0587bb806f39...|[0.06417579634580...|[111749.0,2306.0]|[0.97978168427513...|       0.0|\n|001928523495e52b0...|[0.07079730706755...|    [39.0,1133.0]|[0.03327645051194...|       1.0|\n|001a183f6add73389...|[0.00865004790830...|   [1514.0,441.0]|[0.77442455242966...|       0.0|\n+--------------------+--------------------+-----------------+--------------------+----------+\nonly showing top 20 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"We'll use the famous  User-Defined Functions (UDFs) to  write functions in Python and use them when writing Spark SQL queries. In order to get the probability vector as described in the function, I just did some logical operations to get the desired probability vector.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.types import DoubleType, IntegerType\nfrom pyspark.sql.functions import udf, col\n\ndef extract_prob_0(v):\n    try:\n        return float(v[0]) # Your VectorUDT is of length 2\n    except ValueError:\n        return None\n\ndef extract_prob_1(v):\n    try:\n        return float(v[1])  # Your VectorUDT is of length 2\n    except ValueError:\n        return None\n\n\nextract_prob0_udf = udf(extract_prob_0, DoubleType())\nextract_prob1_udf = udf(extract_prob_1, DoubleType())\n\n\n\nprobabilities = (test_pred.select(\"customer_ID\",\"probability\").withColumn(\"prob_0\", extract_prob0_udf(col(\"probability\")))\n                                                             .withColumn(\"prob_1\", extract_prob1_udf(col(\"probability\"))))","metadata":{"execution":{"iopub.status.busy":"2022-08-07T07:00:21.315951Z","iopub.execute_input":"2022-08-07T07:00:21.316496Z","iopub.status.idle":"2022-08-07T07:00:21.457810Z","shell.execute_reply.started":"2022-08-07T07:00:21.316435Z","shell.execute_reply":"2022-08-07T07:00:21.456845Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"probabilities.select(\"prob_0\",\"prob_1\").show(5)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T07:00:21.458753Z","iopub.execute_input":"2022-08-07T07:00:21.459065Z","iopub.status.idle":"2022-08-07T07:02:25.016243Z","shell.execute_reply.started":"2022-08-07T07:00:21.459036Z","shell.execute_reply":"2022-08-07T07:02:25.014774Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"[Stage 185:=====================================================> (51 + 1) / 52]\r","output_type":"stream"},{"name":"stdout","text":"22/08/07 07:02:21 WARN DAGScheduler: Broadcasting large task binary with size 1017.0 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 191:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+------------------+-------------------+\n|            prob_0|             prob_1|\n+------------------+-------------------+\n| 0.825405554028045|0.17459444597195492|\n|0.6171180075113659|0.38288199248863414|\n|0.9797816842751305|0.02021831572486958|\n|0.9797816842751305|0.02021831572486958|\n|0.9797816842751305|0.02021831572486958|\n+------------------+-------------------+\nonly showing top 5 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"### Some SparkSQL","metadata":{}},{"cell_type":"code","source":"sqlContext = SQLContext(spark)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T07:02:25.017888Z","iopub.execute_input":"2022-08-07T07:02:25.018389Z","iopub.status.idle":"2022-08-07T07:02:25.047864Z","shell.execute_reply.started":"2022-08-07T07:02:25.018339Z","shell.execute_reply":"2022-08-07T07:02:25.044671Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pyspark/sql/context.py:114: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"probabilities.createOrReplaceTempView(\"tmp_probabilities\")\n\nsample_submission = sqlContext.sql(\n\"\"\"\nSELECT \ncustomer_ID,\nCASE WHEN prob_0 > prob_1 THEN 1-prob_0\n          ELSE prob_1\nEND as prediction\nFROM tmp_probabilities\n\"\"\"\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T07:02:25.049531Z","iopub.execute_input":"2022-08-07T07:02:25.050248Z","iopub.status.idle":"2022-08-07T07:02:25.312031Z","shell.execute_reply.started":"2022-08-07T07:02:25.050180Z","shell.execute_reply":"2022-08-07T07:02:25.310973Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"sample_submission.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T07:02:25.313456Z","iopub.execute_input":"2022-08-07T07:02:25.313801Z","iopub.status.idle":"2022-08-07T07:04:43.827141Z","shell.execute_reply.started":"2022-08-07T07:02:25.313772Z","shell.execute_reply":"2022-08-07T07:04:43.823765Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"[Stage 196:=====================================================> (51 + 1) / 52]\r","output_type":"stream"},{"name":"stdout","text":"22/08/07 07:04:41 WARN DAGScheduler: Broadcasting large task binary with size 1018.4 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 202:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"+--------------------+-------------------+\n|         customer_ID|         prediction|\n+--------------------+-------------------+\n|0000210045da4f81e...|0.17459444597195495|\n|00003b41e58ede33b...| 0.3828819924886341|\n|00004ffe6e01e1b68...|0.02021831572486954|\n|00031400ff14b7d1f...|0.02021831572486954|\n|0003e5c8c3d0be014...|0.02021831572486954|\n+--------------------+-------------------+\nonly showing top 5 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Some sanity checks","metadata":{}},{"cell_type":"code","source":"sample_submission.groupBy(\"customer_ID\").agg(F.count(\"customer_ID\")).count() == sample_submission.count()","metadata":{"execution":{"iopub.status.busy":"2022-08-07T07:04:43.832725Z","iopub.execute_input":"2022-08-07T07:04:43.833416Z","iopub.status.idle":"2022-08-07T07:04:59.242296Z","shell.execute_reply.started":"2022-08-07T07:04:43.833360Z","shell.execute_reply":"2022-08-07T07:04:59.240924Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"sample_submission.select(\"prediction\").agg(F.max(\"prediction\"),F.min(\"prediction\")).show()","metadata":{"execution":{"iopub.status.busy":"2022-08-07T07:04:59.244685Z","iopub.execute_input":"2022-08-07T07:04:59.245230Z","iopub.status.idle":"2022-08-07T07:07:56.275378Z","shell.execute_reply.started":"2022-08-07T07:04:59.245152Z","shell.execute_reply":"2022-08-07T07:07:56.273959Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"[Stage 219:=====================================================> (51 + 1) / 52]\r","output_type":"stream"},{"name":"stdout","text":"22/08/07 07:07:17 WARN DAGScheduler: Broadcasting large task binary with size 1124.4 KiB\n","output_type":"stream"},{"name":"stderr","text":"[Stage 225:====================================================>  (19 + 1) / 20]\r","output_type":"stream"},{"name":"stdout","text":"+---------------+---------------+\n|max(prediction)|min(prediction)|\n+---------------+---------------+\n|            1.0|            0.0|\n+---------------+---------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# Convert sparkDataframe to pandas Dataframe ( we collect all the RDD's in the edge node )\nfinal_submission_df = sample_submission.toPandas()","metadata":{"execution":{"iopub.status.busy":"2022-08-07T07:07:56.280040Z","iopub.execute_input":"2022-08-07T07:07:56.282142Z","iopub.status.idle":"2022-08-07T07:10:56.096056Z","shell.execute_reply.started":"2022-08-07T07:07:56.282070Z","shell.execute_reply":"2022-08-07T07:10:56.094971Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"[Stage 237:=====================================================> (51 + 1) / 52]\r","output_type":"stream"},{"name":"stdout","text":"22/08/07 07:10:13 WARN DAGScheduler: Broadcasting large task binary with size 1018.3 KiB\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"target_path = \"/kaggle/working/sample_submission.csv\"\n\nfinal_submission = final_submission_df.to_csv(target_path, index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-07T07:10:56.098982Z","iopub.execute_input":"2022-08-07T07:10:56.099399Z","iopub.status.idle":"2022-08-07T07:10:59.400370Z","shell.execute_reply.started":"2022-08-07T07:10:56.099363Z","shell.execute_reply":"2022-08-07T07:10:59.399260Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"final_submission_df","metadata":{"execution":{"iopub.status.busy":"2022-08-07T07:10:59.401684Z","iopub.execute_input":"2022-08-07T07:10:59.402009Z","iopub.status.idle":"2022-08-07T07:10:59.434814Z","shell.execute_reply.started":"2022-08-07T07:10:59.401980Z","shell.execute_reply":"2022-08-07T07:10:59.433657Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                              customer_ID  prediction\n0       0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...    0.174594\n1       00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...    0.382882\n2       00004ffe6e01e1b688170bbd108da8351bc4c316eacfef...    0.020218\n3       00031400ff14b7d1f113668cc7397b1ebd9c46cce1d14f...    0.020218\n4       0003e5c8c3d0be014d653748cfd93a26be994a5508e1c1...    0.020218\n...                                                   ...         ...\n924616  fff64f5763dd3eb07081f360cba18d82aea1487cbf292c...    0.020218\n924617  fffa8409c201b2a7c6ee5694965d3372d3bbe4c25f8bf3...    0.810811\n924618  fffb015266e480ba5eb8d8297ca41f2fc75cd4d369306f...    0.955357\n924619  fffdf0264d14645b3e369401f5a89242139d2a7aa317b5...    0.020218\n924620  ffffd61f098cc056dbd7d2a21380c4804bbfe60856f475...    0.952128\n\n[924621 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_ID</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...</td>\n      <td>0.174594</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...</td>\n      <td>0.382882</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00004ffe6e01e1b688170bbd108da8351bc4c316eacfef...</td>\n      <td>0.020218</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00031400ff14b7d1f113668cc7397b1ebd9c46cce1d14f...</td>\n      <td>0.020218</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0003e5c8c3d0be014d653748cfd93a26be994a5508e1c1...</td>\n      <td>0.020218</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>924616</th>\n      <td>fff64f5763dd3eb07081f360cba18d82aea1487cbf292c...</td>\n      <td>0.020218</td>\n    </tr>\n    <tr>\n      <th>924617</th>\n      <td>fffa8409c201b2a7c6ee5694965d3372d3bbe4c25f8bf3...</td>\n      <td>0.810811</td>\n    </tr>\n    <tr>\n      <th>924618</th>\n      <td>fffb015266e480ba5eb8d8297ca41f2fc75cd4d369306f...</td>\n      <td>0.955357</td>\n    </tr>\n    <tr>\n      <th>924619</th>\n      <td>fffdf0264d14645b3e369401f5a89242139d2a7aa317b5...</td>\n      <td>0.020218</td>\n    </tr>\n    <tr>\n      <th>924620</th>\n      <td>ffffd61f098cc056dbd7d2a21380c4804bbfe60856f475...</td>\n      <td>0.952128</td>\n    </tr>\n  </tbody>\n</table>\n<p>924621 rows × 2 columns</p>\n</div>"},"metadata":{}}]}]}